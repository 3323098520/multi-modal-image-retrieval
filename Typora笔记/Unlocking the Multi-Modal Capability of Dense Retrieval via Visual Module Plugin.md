# Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin

- **作者**：Tianshuo Zhou 等（东北大学、卡内基梅隆大学、清华大学）
- **发表来源**：ACL 2024（计算语言学协会年会）
- 视觉插件增强的多模态密集检索研究



## 研究背景

现有*多模态检索*存在两大局限：一是模态鸿沟，现有模型（如 UniVL-DR）用 CLIP 分别编码图像和文本，导致表示空间异构，需额外图像文本化（verbalization）缓解，且无法复用文本密集检索模型（如 T5-ANCE）的优势；二是缺乏大规模科学 / 网页领域多模态基准，通用数据集难以覆盖真实检索场景。此外，视觉语言模型（VLMs）多聚焦语义对齐，而非检索导向的表示学习，检索性能不足。



## 主要工作

1. **提出 MARVEL 框架**：以文本密集检索模型 T5-ANCE 为基础，引入 CLIP 视觉编码器作为 “插件”，通过视觉模块预训练和模态平衡微调，实现多模态统一编码，打破模态鸿沟，复用文本检索的匹配知识。
2. **构建大规模多模态基准 ClueWeb22-MM**：基于 ClueWeb22 网页数据集，以锚文本为查询，提取链接网页中的图像和文本，构建含 9 万查询、73 万多模态文档的基准，支持网页场景多模态检索。
3. **实验验证**：在 WebQA（多模态 QA）和 ClueWeb22-MM 上，对比单模态检索、分治式检索（如 CLIP-DPR+BM25）、通用多模态检索（UniVL-DR），证明 MARVEL 在 MRR@10 上提升 2%~7%，且效率与文本检索模型相当。



## 总结

MARVEL 的核心创新是 “视觉插件 + 文本检索模型” 的轻量化整合，既复用了文本密集检索的成熟技术，又打破了模态鸿沟，实现高效多模态检索。其构建的 ClueWeb22-MM 基准为网页场景多模态检索提供了评估基础。