# Visualized Text Embedding For Universal Multi-Modal Retrieval

- **作者**：Junjie Zhou 等（北京邮电大学、北京人工智能研究院、香港理工大学）
- **发表来源**：ACL 2024（计算语言学协会年会）
- 通用多模态检索的可视化文本嵌入研究



现有多模态检索存在三大局限：一是检索器文本导向，无法有效处理视觉信息；二是视觉语言模型（如 CLIP、BLIP）的文本表示能力远弱于专用文本嵌入模型（如 BGE、E5）；三是缺乏高质量多模态训练数据，现有数据集依赖人工标注，规模有限且泛化性差。此外，现有模型难以处理 “图像 - 文本组合查询”“多模态文档检索” 等复杂场景。

## 基于上述背景，Junjie Zhou 等的核心工作

1. **提出 VISTA 模型架构**：以强文本编码器（BGE-Base-v1.5）为基础，引入 ViT 作为 “图像 Tokenizer”，将图像编码为文本编码器可理解的视觉 token，实现图像、文本、多模态组合数据的统一编码，保留文本嵌入能力。
2. **生成高质量多模态数据集**：设计 IT2I（图像 + 文本→图像）和 T2IT（文本→图像 + 文本）两种自动生成策略，无需人工标注，构建 307K IT2I 样本和 213K T2IT 样本，覆盖组合查询和多模态文档场景。
3. **两阶段训练策略**：先通过跨模态对比训练对齐视觉 token 与文本编码器，再通过多模态训练提升复杂场景检索能力，在 WebQA、CIRR、FashionIQ 等 5 个基准上实现零样本和微调 SOTA。



## 阅读总结

VISTA 的核心突破是 “以文本编码器为核心，用 ViT 将图像转化为视觉 token”，既保留了专用文本嵌入模型的高精度，又实现了多模态统一编码，解决了现有模型 “文本能力弱”“模态融合浅” 的问题。其自动生成数据集的策略也为解决多模态数据稀缺提供了新思路。