# Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning

- 多模态检索增强的生成式常识推理研究
- **作者**：Wanqing Cui 等（中国科学院计算技术研究所、中国科学院大学）
- **发表来源**：NeurIPS 2024（神经信息处理系统大会）



## 解决的核心问题

大语言模型（LLMs）存在常识推理能力不足的问题，根源是 “报告偏差”—— 常识信息在文本中记录远少于其实际存在，导致预训练难以覆盖足够常识。现有检索增强方法多依赖文本检索，忽略图像天然携带的客观常识信息（如 “树用装饰品装饰，而非音乐”）；且存在两大挑战：一是文本型 LLM 难以有效提取多模态检索结果的知识，二是检索结果质量波动大，LLM 易忽略有效信息或盲目信任噪声。



## 核心工作（做了什么）

1. **提出 MORE 多模态检索增强框架**：融合文本和图像检索结果，增强 LLM 的常识推理能力，支持单模态（文本 / 图像）和多模态 LLM（如 BLIP2），适用于生成式常识推理任务（如 CommonGen）。
2. **设计插件式多模态整合器**：通过跨注意力机制提取检索结果中的有用信息，解决模态差异问题；对文本型 LLM，用 BLIP2 的 Q-Former 作为多模态编码器，将图像 / 文本检索结果映射到 LLM 的表示空间。
3. **提出稳健训练策略**：引入 “查询 dropout”（随机屏蔽 LLM 的查询输入，迫使模型利用检索结果）和 “噪声 RA 输入”（随机替换检索结果为无关信息，引导模型忽略噪声），平衡检索结果的利用与信任度。
4. **实验验证**：在 CommonGen 数据集（生成含指定概念的常识句子）上，对比 GPT-3.5/4、文本检索增强基线（FiD、Prepend）、视觉增强基线（VisCTG），证明 MORE 在 Bleu4、SPICE 等指标上的优越性。



## 阅读总结

本文的核心突破是将图像模态引入检索增强常识推理，通过 “插件式整合器 + 稳健训练策略” 解决了多模态知识提取和噪声处理问题。其价值在于：一是验证了图像对常识推理的补充作用，二是提出的框架可适配不同类型 LLM，无需修改模型结构。

tips：可借鉴 MORE 的 “多模态检索 + 跨注意力整合” 思路，将地图的文字标注（文本）、形态特征（图像）检索结果融合，辅助 LLM 理解地图隐含语义，提升检索精准度。